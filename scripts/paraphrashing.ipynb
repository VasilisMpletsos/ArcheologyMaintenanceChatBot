{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include the necessary libraries\n",
    "import torch\n",
    "import warnings\n",
    "import yaml\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"./FineTunedParrotParaphraser\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"./FineTunedParrotParaphraser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read a yaml file\n",
    "def read_yaml_file(file_path):\n",
    "    with open(file_path, 'r') as stream:\n",
    "        try:\n",
    "            return yaml.safe_load(stream)\n",
    "        except yaml.YAMLError as exc:\n",
    "            print(exc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean phrases\n",
    "def clean_phrases(examples):\n",
    "    phrases = examples.split('\\n')\n",
    "    # remove empty strings\n",
    "    phrases = [phrase for phrase in phrases if phrase != '']\n",
    "    # remove dash and space\n",
    "    phrases = [phrase[2:] for phrase in phrases]\n",
    "    return phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking at pebbles_degredations.yml\n",
      "Number of paraphrased phrases for inform_pebbles_with_cracks: 18\n",
      "Number of paraphrased phrases for inform_pebbles_with_sulfation: 18\n",
      "Number of paraphrased phrases for inform_pebbles_with_cavities: 30\n",
      "Number of paraphrased phrases for inform_pebbles_with_crystal_decomposition: 27\n",
      "Number of paraphrased phrases for inform_pebbles_with_disintegration: 15\n",
      "Number of paraphrased phrases for inform_pebbles_with_infection: 69\n"
     ]
    }
   ],
   "source": [
    "task_prefix = \"paraphrase: \"\n",
    "\n",
    "# create paraphrased folder\n",
    "if not os.path.exists('../data/nlu/paraphrased'):\n",
    "    os.makedirs('../data/nlu/paraphrased')\n",
    "    \n",
    "# create paraphrased folder\n",
    "if not os.path.exists('../data/responses/paraphrased'):\n",
    "    os.makedirs('../data/responses/paraphrased')\n",
    "\n",
    "# iterate over the nlu files\n",
    "for file in os.listdir('../data/nlu/needs_paraphrasing'):\n",
    "    # if file is a yaml file\n",
    "    if file.endswith('.yml'):\n",
    "        # print the file name\n",
    "        print(f'Looking at {file}')\n",
    "        # read yaml file\n",
    "        data = read_yaml_file('../data/nlu/needs_paraphrasing/' + file)\n",
    "        # file name\n",
    "        file_name = file.split('.')[0]\n",
    "        if not os.path.exists(f'../data/nlu/paraphrased/{file_name}'):\n",
    "            os.makedirs(f'../data/nlu/paraphrased/{file_name}')\n",
    "        # for each intent in nlu\n",
    "        for row in data['nlu']:\n",
    "            intent = row['intent']\n",
    "            examples = row['examples']\n",
    "            # get phrases\n",
    "            phrases = clean_phrases(examples)\n",
    "            initial_len = len(phrases)\n",
    "            final_phrases = []\n",
    "            for phrase in phrases:\n",
    "                phrase = tokenizer([task_prefix + phrase], return_tensors=\"pt\", padding=True)\n",
    "                para_phrases = model.generate(\n",
    "                    phrase['input_ids'],\n",
    "                    do_sample=False, \n",
    "                    max_length=1026, \n",
    "                    num_beams = 16,\n",
    "                    num_beam_groups = 4,\n",
    "                    diversity_penalty = 4.0,\n",
    "                    early_stopping=True,\n",
    "                    num_return_sequences=4\n",
    "                    )\n",
    "                # final_phrases.append(phrase)\n",
    "                para_phrases = tokenizer.batch_decode(para_phrases, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "                for para_phrase in para_phrases:\n",
    "                    # if score is bigger than 10 append the paraphrased phrase to the list\n",
    "                    final_phrases.append(para_phrase)\n",
    "            final_len = len(final_phrases)\n",
    "            # print the number of paraphrased phrases\n",
    "            print(f\"Number of paraphrased phrases for {intent}: {final_len - initial_len}\")\n",
    "            # capitalize the first letter of the phrases\n",
    "            final_phrases = [response[0].capitalize() + response[1:] for response in final_phrases]\n",
    "            final_phrases = ['- ' + phrase for phrase in final_phrases]\n",
    "            updated_phrases = '\\n'.join(final_phrases)\n",
    "            # write to data.txt\n",
    "            with open(f\"../data/nlu/paraphrased/{file_name}/{intent}.txt\", 'w') as f:\n",
    "                f.write(updated_phrases)\n",
    "                \n",
    "                \n",
    "# # iterate over the responses files\n",
    "# for file in os.listdir('../data/responses/needs_paraphrasing'):\n",
    "#     # if file is a yaml file\n",
    "#     if file.endswith('.yml'):\n",
    "#         # print the file name\n",
    "#         print(f'Looking at {file}')\n",
    "#         # read yaml file\n",
    "#         data = read_yaml_file('../data/responses/needs_paraphrasing/' + file)\n",
    "#         # file name\n",
    "#         file_name = file.split('.')[0]\n",
    "#         if not os.path.exists(f'../data/responses/paraphrased/{file_name}'):\n",
    "#             os.makedirs(f'../data/responses/paraphrased/{file_name}')\n",
    "#         # for each intent in nlu\n",
    "#         for row in data['responses']:\n",
    "#             final_responses = []\n",
    "#             initial_len = len(data['responses'][row])\n",
    "#             for sentence in data['responses'][row]:\n",
    "#                 sentence = sentence['text']\n",
    "#                 final_responses.append(sentence)\n",
    "#                 para_phrases = model.augment(input_phrase=sentence)\n",
    "#                 for para_phrase in para_phrases:\n",
    "#                     # if score is bigger than 10 append the paraphrased phrase to the list\n",
    "#                     if para_phrase[1] > 10:\n",
    "#                         # append the paraphrased phrase to the list\n",
    "#                         final_responses.append(para_phrase[0])\n",
    "#             final_len = len(final_responses)\n",
    "#             # print the number of paraphrased phrases\n",
    "#             print(f\"Number of paraphrased phrases for {row}: {final_len - initial_len}\")\n",
    "#             # capitalize the first letter of the phrases\n",
    "#             final_responses = [response[0].capitalize() + response[1:] for response in final_responses]\n",
    "#             # add the - text: part that is needed\n",
    "#             final_responses = ['- text: ' + response for response in final_responses]\n",
    "#             updated_responses = '\\n'.join(final_responses)\n",
    "#             # write to data.txt\n",
    "#             with open(f\"../data/responses/paraphrased/{file_name}/{row}.txt\", 'w') as f:\n",
    "#                 f.write(updated_responses)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete the needs_paraphrasing files\n",
    "for file in os.listdir('../data/nlu/needs_paraphrasing'):\n",
    "    os.remove(f'../data/nlu/needs_paraphrasing/{file}')\n",
    "    \n",
    "for file in os.listdir('../data/responses/needs_paraphrasing'):\n",
    "    os.remove(f'../data/responses/needs_paraphrasing/{file}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "guide",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
