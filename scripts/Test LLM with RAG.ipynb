{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vmpletsos\\Anaconda3\\envs\\guide\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel, pipeline, PreTrainedTokenizer, AutoModelForCausalLM\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers.util import semantic_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "INTRO_BLURB = (\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\")\n",
    "\n",
    "# To be added as special tokens\n",
    "INSTRUCTION_KEY = \"### Instruction:\"\n",
    "INPUT_KEY = \"Input:\"\n",
    "RESPONSE_KEY = \"### Response:\"\n",
    "END_KEY = \"### End\"\n",
    "RESPONSE_KEY_NL = f\"{RESPONSE_KEY}\\n\"\n",
    "\n",
    "\n",
    "PROMPT_FOR_GENERATION_FORMAT = \"\"\"{intro}\n",
    "\n",
    "{instruction_key}\n",
    "{instruction}\n",
    "\n",
    "{response_key}\n",
    "\"\"\".format(\n",
    "    intro=INTRO_BLURB,\n",
    "    instruction_key=INSTRUCTION_KEY,\n",
    "    instruction=\"{instruction}\",\n",
    "    response_key=RESPONSE_KEY,\n",
    ")\n",
    "\n",
    "\n",
    "# This is the prompt that is used for generating responses using an already trained model.  It ends with the response\n",
    "# key, where the job of the model is to provide the completion that follows it (i.e. the response itself).\n",
    "PROMPT_FOR_GENERATION_FORMAT_WITH_INPUT = \"\"\"{intro}\n",
    "\n",
    "{instruction_key}\n",
    "{instruction}\n",
    "\n",
    "{input_key}\n",
    "{context}\n",
    "\n",
    "{response_key}\n",
    "\"\"\".format(\n",
    "    intro=INTRO_BLURB,\n",
    "    instruction_key=INSTRUCTION_KEY,\n",
    "    instruction=\"{instruction}\",\n",
    "    input_key=INPUT_KEY,\n",
    "    context=\"{context}\",\n",
    "    response_key=RESPONSE_KEY,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_special_token_id(tokenizer: PreTrainedTokenizer, key: str) -> int:\n",
    "    \"\"\"Gets the token ID for a given string that has been added to the tokenizer as a special token.\n",
    "    When training, we configure the tokenizer so that the sequences like \"### Instruction:\" and \"### End\" are\n",
    "    treated specially and converted to a single, new token.  This retrieves the token ID each of these keys map to.\n",
    "    Args:\n",
    "        tokenizer (PreTrainedTokenizer): the tokenizer\n",
    "        key (str): the key to convert to a single token\n",
    "    Raises:\n",
    "        RuntimeError: if more than one ID was generated\n",
    "    Returns:\n",
    "        int: the token ID for the given key\n",
    "    \"\"\"\n",
    "    token_ids = tokenizer.encode(key)\n",
    "    if len(token_ids) > 1:\n",
    "        print(f\"Expected only a single token for '{key}' but found {token_ids}\")\n",
    "    return token_ids[0]\n",
    "\n",
    "def preprocess(tokenizer, instruction_text, context_text=None):\n",
    "    instruction = \"Answer the following question only with the provided input. If no answer is found tell that you cannot answer based on this context. \" + instruction_text\n",
    "    if context_text:\n",
    "        prompt_text = PROMPT_FOR_GENERATION_FORMAT_WITH_INPUT.format(instruction=instruction, context=context_text)\n",
    "    else:\n",
    "        prompt_text = PROMPT_FOR_GENERATION_FORMAT.format(instruction=instruction)\n",
    "\n",
    "    inputs = tokenizer(prompt_text, return_tensors=\"pt\",)\n",
    "    inputs[\"prompt_text\"] = prompt_text\n",
    "    inputs[\"instruction_text\"] = instruction_text\n",
    "    inputs[\"context_text\"] = context_text\n",
    "    return inputs\n",
    "\n",
    "def forward(model, tokenizer, model_inputs, max_length=256):\n",
    "    input_ids = model_inputs[\"input_ids\"]\n",
    "    attention_mask = model_inputs.get(\"attention_mask\", None)\n",
    "\n",
    "    if input_ids.shape[1] == 0:\n",
    "        input_ids = None\n",
    "        attention_mask = None\n",
    "        in_b = 1\n",
    "    else:\n",
    "        in_b = input_ids.shape[0]\n",
    "\n",
    "    generated_sequence = model.generate(\n",
    "        input_ids=input_ids.to(model.device),\n",
    "        attention_mask=attention_mask.to(model.device),\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        max_length=max_length\n",
    "    )\n",
    "\n",
    "    out_b = generated_sequence.shape[0]\n",
    "    generated_sequence = generated_sequence.reshape(in_b, out_b // in_b, *generated_sequence.shape[1:])\n",
    "    instruction_text = model_inputs.pop(\"instruction_text\", None)\n",
    "\n",
    "    return {\n",
    "        \"generated_sequence\": generated_sequence, \n",
    "        \"input_ids\": input_ids,\n",
    "    }\n",
    "\n",
    "\n",
    "def postprocess(tokenizer, model_outputs):\n",
    "    response_key_token_id = get_special_token_id(tokenizer, RESPONSE_KEY_NL)\n",
    "    end_key_token_id = get_special_token_id(tokenizer, END_KEY)\n",
    "    generated_sequence = model_outputs[\"generated_sequence\"][0]\n",
    "    \n",
    "    # send it to cpu\n",
    "    generated_sequence = generated_sequence.cpu()\n",
    "    generated_sequence = generated_sequence.numpy().tolist()\n",
    "    records = []\n",
    "\n",
    "    for sequence in generated_sequence:\n",
    "        decoded = None\n",
    "\n",
    "        try:\n",
    "            response_pos = sequence.index(response_key_token_id)\n",
    "        except ValueError:\n",
    "            print(f\"Could not find response key {response_key_token_id} in: {sequence}\")\n",
    "            response_pos = None\n",
    "\n",
    "        if response_pos:\n",
    "            try:\n",
    "                end_pos = sequence.index(end_key_token_id)\n",
    "            except ValueError:\n",
    "                print(\"Could not find end key, the output is truncated!\")\n",
    "                end_pos = None\n",
    "                \n",
    "            if end_pos:\n",
    "                decoded = tokenizer.decode(sequence[response_pos + 1 : end_pos], skip_special_tokens=True).strip()\n",
    "            else:\n",
    "                decoded = \"Sorry i cannot answer this question\";         \n",
    "            \n",
    "        rec = {\"generated_text\": decoded}\n",
    "        records.append(rec)\n",
    "    return records\n",
    "\n",
    "def get_model_tokenizer(pretrained_model_name_or_path):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path);\n",
    "    model = model = AutoModelForCausalLM.from_pretrained(\n",
    "        pretrained_model_name_or_path, \n",
    "        torch_dtype = torch.bfloat16,\n",
    "    );\n",
    "    model.resize_token_embeddings(len(tokenizer));\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the LLM Dolly v2 3b model with its tokenizer\n",
    "LLM_model, LLM_tokenizer = get_model_tokenizer(pretrained_model_name_or_path = \"./FineTunedDollyV2\");\n",
    "LLM_model = LLM_model.to('cuda');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load similarity model\n",
    "similarity_tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L12-v2')\n",
    "similarity_model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L12-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    # First element of model_output contains all token embeddings\n",
    "    token_embeddings = model_output[0]\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     Loaded knowledge base!\n"
     ]
    }
   ],
   "source": [
    "# Read the extracted questions csv\n",
    "df = pd.read_csv('./new_passages.csv')\n",
    "passages = df.passages.to_list()\n",
    "encoded_input = similarity_tokenizer(passages, padding=True, truncation=True, return_tensors='pt')\n",
    "with torch.no_grad():\n",
    "    model_output = similarity_model(**encoded_input)\n",
    "# Perform pooling\n",
    "sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "sentence_embeddings = sentence_embeddings.detach().numpy()\n",
    "print(\"INFO:     Loaded knowledge base!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Construction marble is a stone composed of calcite, dolomite or serpentine. It is harder, more glossy and stain resistant than the original surface.\n"
     ]
    }
   ],
   "source": [
    "query = 'Can we polish marble with a material?'\n",
    "tokenized_query = similarity_tokenizer(query, padding=True, truncation=True, return_tensors='pt');\n",
    "embedded_query = similarity_model(**tokenized_query);\n",
    "question_embeddings = mean_pooling(embedded_query, tokenized_query['attention_mask']);\n",
    "question_embeddings = question_embeddings.detach().numpy();\n",
    "retrieved = semantic_search(question_embeddings, sentence_embeddings, top_k=2);\n",
    "# retrieved = retrieved_all[0][0];\n",
    "# max_pos = retrieved['corpus_id'];\n",
    "# max_score = retrieved['score'];\n",
    "# context = passages[max_pos];\n",
    "context = []\n",
    "for row in retrieved[0]:\n",
    "    context.append(passages[row['corpus_id']]);\n",
    "context = '\\n'.join(context)\n",
    "pre_process_result = preprocess(LLM_tokenizer, query, context);\n",
    "model_result = forward(LLM_model, LLM_tokenizer, pre_process_result);\n",
    "final_output = postprocess(LLM_tokenizer, model_result);\n",
    "response = final_output[0]['generated_text'];\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "guide",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
