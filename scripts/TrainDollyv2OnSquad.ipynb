{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d3f3ade8-1c53-4b7e-9044-35f1c82bf5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import bitsandbytes as bnb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftConfig,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    ")\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b00b3d00-fae2-4d66-9760-5e282a665274",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"databricks/dolly-v2-3b\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    load_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "model =AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=bnb_config,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "017d95e2-fcc0-4700-b224-9d94afada028",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0b34411-e6d5-49de-a5e2-60784a1adb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"query_key_value\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4cd8c71-ce85-4ae7-9c58-0eaafacde607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be added as special tokens\n",
    "INSTRUCTION_KEY = \"### Instruction:\"\n",
    "INPUT_KEY = \"Input:\"\n",
    "RESPONSE_KEY = \"### Response:\"\n",
    "END_KEY = \"### End\"\n",
    "RESPONSE_KEY_NL = f\"{RESPONSE_KEY}\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c7793fb-3e9e-440e-bbfa-af3dc3423915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training prompt that does not contain an input string.\n",
    "INTRO_BLURB = (\n",
    "    \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
    ")\n",
    "\n",
    "PROMPT_NO_INPUT_FORMAT = \"\"\"{intro}\n",
    "\n",
    "{instruction_key}\n",
    "{instruction}\n",
    "\n",
    "{response_key}\n",
    "{response}\n",
    "\n",
    "{end_key}\n",
    "\"\"\".format(\n",
    "    intro=INTRO_BLURB,\n",
    "    instruction_key=INSTRUCTION_KEY,\n",
    "    instruction=\"{instruction}\",\n",
    "    response_key=RESPONSE_KEY,\n",
    "    response=\"{response}\",\n",
    "    end_key=END_KEY,\n",
    ")\n",
    "\n",
    "\n",
    "# This is the prompt that is used for generating responses using an already trained model.  It ends with the response\n",
    "# key, where the job of the model is to provide the completion that follows it (i.e. the response itself).\n",
    "PROMPT_WITH_INPUT_FORMAT = \"\"\"{intro}\n",
    "\n",
    "{instruction_key}\n",
    "{instruction}\n",
    "\n",
    "{input_key}\n",
    "{context}\n",
    "\n",
    "{response_key}\n",
    "{response}\n",
    "\n",
    "{end_key}\n",
    "\"\"\".format(\n",
    "    intro=INTRO_BLURB,\n",
    "    instruction_key=INSTRUCTION_KEY,\n",
    "    instruction=\"{instruction}\",\n",
    "    input_key=INPUT_KEY,\n",
    "    context=\"{context}\",\n",
    "    response_key=RESPONSE_KEY,\n",
    "    response=\"{response}\",\n",
    "    end_key=END_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd203c8d-9820-49ad-bcf5-bed9288afdb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"squad_v2\")\n",
    "dataset_train = dataset[\"train\"].select(range(3000))\n",
    "dataset_test = dataset[\"validation\"].select(range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44b92616-a80b-41a2-8697-d06a8c219dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_for_LLM(rec):\n",
    "    instruction = rec[\"question\"]\n",
    "    instruction = \"Answer the following question only with the provided input. If no answer is found tell that you cannot answer based on this context. \" + instruction\n",
    "    try:\n",
    "        response = rec[\"answers\"]['text'][0]\n",
    "    except:\n",
    "        # print(rec['answers'])\n",
    "        response = \"I cannot answer this question based on this context.\"\n",
    "    context = rec.get(\"context\")\n",
    "    if context:\n",
    "        rec[\"text\"] = PROMPT_WITH_INPUT_FORMAT.format(\n",
    "            instruction=instruction, \n",
    "            response=response, \n",
    "            context=context\n",
    "        )\n",
    "    else:\n",
    "        rec[\"text\"] = PROMPT_NO_INPUT_FORMAT.format(\n",
    "            instruction=instruction, \n",
    "            response=response\n",
    "        )\n",
    "    return rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c75579e0-7de2-4568-891a-9e81063b1302",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = dataset_train.map(format_for_LLM)\n",
    "dataset_test = dataset_test.map(format_for_LLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f11916e-e81c-490a-9bb7-4856aa11cf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_tokenize_text(rec):\n",
    "  tokenized_full_prompt = tokenizer(rec['text'], padding=True, truncation=True)\n",
    "  return tokenized_full_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49132858-fc72-4d75-81c7-10c8e48623de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_prompt(data_point):\n",
    "#   return f\"\"\"\n",
    "# <Human>: {data_point[\"Context\"]}\n",
    "# <AI>: {data_point[\"Response\"]}\n",
    "#   \"\"\".strip()\n",
    "\n",
    "# def generate_and_tokenize_prompt(data_point):\n",
    "#   full_prompt = generate_prompt(data_point)\n",
    "#   tokenized_full_prompt = tokenizer(full_prompt, padding=True, truncation=True)\n",
    "#   return tokenized_full_prompt\n",
    "    \n",
    "# dataset_name = 'Amod/mental_health_counseling_conversations'\n",
    "# dataset = load_dataset(dataset_name, split=\"train\")\n",
    "\n",
    "# dataset = dataset.shuffle().map(generate_and_tokenize_text)\n",
    "\n",
    "dataset_train = dataset_train.map(generate_and_tokenize_text)\n",
    "dataset_test = dataset_test.map(generate_and_tokenize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9f33bbe-714f-4ca6-981d-8cb2396050f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "872b8c5c-1190-432d-9157-d06e89cc2226",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f06debef69f4655a180def57fa4416c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba2a5d24dc40457798972bbb13df1c5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make sure we don't have any truncated records, as this would mean the end keyword is missing.\n",
    "dataset_train = dataset_train.filter(lambda rec: len(rec[\"input_ids\"]) < max_length)\n",
    "dataset_test = dataset_test.filter(lambda rec: len(rec[\"input_ids\"]) < max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e43bf118-8a3a-4efe-83a0-85162b3779f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'title', 'context', 'question', 'answers', 'text', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 3000\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6738fcb0-3b5b-4e57-b59c-c21aa61e9b4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'title', 'context', 'question', 'answers', 'text', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 100\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "884e6371-51d1-44bb-a258-908f881808d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataCollatorForCompletionOnlyLM(DataCollatorForLanguageModeling):\n",
    "    def torch_call(self, examples):\n",
    "        batch = super().torch_call(examples)\n",
    "\n",
    "        # The prompt ends with the response key plus a newline\n",
    "        response_token_ids = self.tokenizer.encode(RESPONSE_KEY_NL)\n",
    "        labels = batch[\"labels\"].clone()\n",
    "\n",
    "        for i in range(len(examples)):\n",
    "            response_token_ids_start_idx = None\n",
    "            for idx in np.where(batch[\"labels\"][i] == response_token_ids[0])[0]:\n",
    "                response_token_ids_start_idx = idx\n",
    "                break\n",
    "\n",
    "            if response_token_ids_start_idx is None:\n",
    "                raise RuntimeError(\n",
    "                    f'Could not find response key {response_token_ids} in token IDs {batch[\"labels\"][i]}'\n",
    "                )\n",
    "\n",
    "            response_token_ids_end_idx = response_token_ids_start_idx + 1\n",
    "\n",
    "            # loss function ignore all tokens up through the end of the response key\n",
    "            labels[i, :response_token_ids_end_idx] = -100\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6f8db174-efde-4277-a027-730109c4d38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForCompletionOnlyLM(tokenizer=tokenizer, mlm=False, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "56e2a617-6219-4166-aa38-30238e425fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = transformers.TrainingArguments(\n",
    "    auto_find_batch_size=True,\n",
    "    num_train_epochs=4,\n",
    "    learning_rate=2e-4,\n",
    "    bf16=True,\n",
    "    save_total_limit=4,\n",
    "    logging_steps=4,\n",
    "    logging_strategy=\"epoch\",\n",
    "    output_dir='./dollytest',\n",
    "    save_strategy='epoch',\n",
    "    do_eval=True,\n",
    "    evaluation_strategy='epoch',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75bc33f-1e10-4ee0-a7ad-c8a5c1520326",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  10/1500 00:07 < 23:44, 1.05 it/s, Epoch 0.02/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset_train,\n",
    "    eval_dataset=dataset_test,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    # data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "model.config.use_cache = False\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bea3bd8-0b63-4d84-9e0e-9355b7deca4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
