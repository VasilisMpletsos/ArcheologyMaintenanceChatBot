{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sys import getsizeof\n",
    "from transformers import AutoTokenizer, AutoModel, pipeline\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from rank_bm25 import BM25Okapi\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from llama_index import SimpleDirectoryReader\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSeq2SeqLM, pipeline\n",
    "from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer, DPRContextEncoder, DPRContextEncoderTokenizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers.util import semantic_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load similarity model\n",
    "# similarity_tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "# similarity_model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "# print(\"Loaded similarity model!\")\n",
    "\n",
    "# # Mean Pooling - Take attention mask into account for correct averaging\n",
    "# def mean_pooling(model_output, attention_mask):\n",
    "#     # First element of model_output contains all token embeddings\n",
    "#     token_embeddings = model_output[0]\n",
    "#     input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "#     return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "# # Read the extracted questions csv\n",
    "# df = pd.read_csv('../actions/new_passages.csv')\n",
    "# passages = df.passages.to_list()\n",
    "# encoded_input = similarity_tokenizer(passages, padding=True, truncation=True, return_tensors='pt')\n",
    "# with torch.no_grad():\n",
    "#     model_output = similarity_model(**encoded_input)\n",
    "# # Perform pooling\n",
    "# sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "# sentence_embeddings = sentence_embeddings.detach().numpy()\n",
    "# print(\"Loaded knowledge base!\")\n",
    "\n",
    "\n",
    "# # Load similarity model\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n",
    "# print(\"Loaded tokenizer!\")\n",
    "# model = DPRContextEncoder.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n",
    "# # model.to(\"cuda\")\n",
    "# print(\"Loaded similarity model!\")\n",
    "\n",
    "# # Read the extracted questions csv\n",
    "# df = pd.read_csv('../actions/new_passages.csv')\n",
    "# passages = df.passages.to_list()\n",
    "# input_ids = tokenizer(passages, return_tensors=\"pt\")[\"input_ids\"]\n",
    "# input_ids = input_ids.to(\"cuda\")\n",
    "# sentence_embeddings = model(input_ids).pooler_output\n",
    "# print(\"Loaded knowledge base!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove with regex the brackets and its content from a phrase\n",
    "def remove_references(text):\n",
    "    # strip sentenece\n",
    "    text = text.lower().strip()\n",
    "    # remove strange characters from documents\n",
    "    text = text.replace('\\xad', '')\n",
    "    # remove dashes\n",
    "    text = text.replace('-', '')\n",
    "    # remove new lines\n",
    "    text = text.replace('\\n', '')\n",
    "    # remove strange combinations\n",
    "    text = text.replace('=-', '')\n",
    "    # remove brackets\n",
    "    text = re.sub(r'\\(\\d+\\)', '', text)\n",
    "    # remove figures\n",
    "    text = re.sub(r'\\(\\w+ \\d+\\)', '', text)\n",
    "    # remove references\n",
    "    return re.sub(r'\\[[\\d\\- ,]+\\]', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader('./Knowledge Base/').load_data();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [document.text for document in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(        \n",
    "    separator = \".\",\n",
    "    chunk_size = 300,\n",
    "    chunk_overlap  = 150,\n",
    "    length_function = len,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 312, which is longer than the specified 300\n",
      "Created a chunk of size 436, which is longer than the specified 300\n",
      "Created a chunk of size 353, which is longer than the specified 300\n",
      "Created a chunk of size 385, which is longer than the specified 300\n",
      "Created a chunk of size 316, which is longer than the specified 300\n",
      "Created a chunk of size 330, which is longer than the specified 300\n",
      "Created a chunk of size 314, which is longer than the specified 300\n",
      "Created a chunk of size 347, which is longer than the specified 300\n",
      "Created a chunk of size 355, which is longer than the specified 300\n",
      "Created a chunk of size 329, which is longer than the specified 300\n",
      "Created a chunk of size 320, which is longer than the specified 300\n",
      "Created a chunk of size 302, which is longer than the specified 300\n",
      "Created a chunk of size 378, which is longer than the specified 300\n",
      "Created a chunk of size 340, which is longer than the specified 300\n",
      "Created a chunk of size 301, which is longer than the specified 300\n",
      "Created a chunk of size 309, which is longer than the specified 300\n",
      "Created a chunk of size 366, which is longer than the specified 300\n",
      "Created a chunk of size 320, which is longer than the specified 300\n",
      "Created a chunk of size 454, which is longer than the specified 300\n",
      "Created a chunk of size 384, which is longer than the specified 300\n",
      "Created a chunk of size 304, which is longer than the specified 300\n",
      "Created a chunk of size 324, which is longer than the specified 300\n",
      "Created a chunk of size 320, which is longer than the specified 300\n",
      "Created a chunk of size 315, which is longer than the specified 300\n"
     ]
    }
   ],
   "source": [
    "texts = text_splitter.create_documents(corpus);\n",
    "texts = [remove_references(sentence.page_content) for sentence in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'passages': texts})\n",
    "df.to_csv('new_passages.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('new_passages.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model from HuggingFace Hub\n",
    "similarity_tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L12-v2')\n",
    "similarity_model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L12-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# questions = df.question.to_list()\n",
    "# Tokenize sentences\n",
    "encoded_input = similarity_tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n",
    "with torch.no_grad():\n",
    "    model_output = similarity_model(**encoded_input)\n",
    "\n",
    "# Perform pooling\n",
    "sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "sentence_embeddings = sentence_embeddings.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentences = [text.split(\" \") for text in texts]\n",
    "bm25 = BM25Okapi(tokenized_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM = pipeline(\n",
    "    model=\"databricks/dolly-v2-3b\", \n",
    "    torch_dtype=torch.bfloat16, \n",
    "    trust_remote_code=True,\n",
    "    return_full_text=True,\n",
    "    device_map=\"auto\",\n",
    "    task=\"text-generation\",\n",
    "    temperature=0.9,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# template for an instrution with no input\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"instruction\"],\n",
    "    template=\"{instruction}\")\n",
    "\n",
    "# template for an instruction with input\n",
    "prompt_with_context = PromptTemplate(\n",
    "    input_variables=[\"instruction\", \"context\"],\n",
    "    template=\"{instruction}\\n\\nInput:\\n{context}\")\n",
    "\n",
    "hf_pipeline = HuggingFacePipeline(pipeline=LLM)\n",
    "\n",
    "llm_chain = LLMChain(llm=hf_pipeline, prompt=prompt)\n",
    "llm_context_chain = LLMChain(llm=hf_pipeline, prompt=prompt_with_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Is marble strong\n",
      "------------- Similarity NN -------------\n",
      "Similarity score: 72.79%\n",
      "Context: the hardness of marble is very high, because the internal structure of the rock is very uniform after longterm natural aging, and the internal stress disappears, so the marble will not be deformed due to temperature, and has strong wear resistance. it is a very popular building material\n",
      "------------- Similarity BM25 -------------\n",
      "Similarity score: 7.102331426879097\n",
      "Top answer: the hardness of marble is very high, because the internal structure of the rock is very uniform after longterm natural aging, and the internal stress disappears, so the marble will not be deformed due to temperature, and has strong wear resistance. it is a very popular building material\n"
     ]
    }
   ],
   "source": [
    "query = \"Is marble strong\"\n",
    "tokenized_query = similarity_tokenizer(query, padding=True, truncation=True, return_tensors='pt')\n",
    "embedded_query = similarity_model(**tokenized_query)\n",
    "question_embeddings = mean_pooling(embedded_query, tokenized_query['attention_mask'])\n",
    "question_embeddings = question_embeddings.detach().numpy()\n",
    "hits = semantic_search(question_embeddings, sentence_embeddings, top_k=1)\n",
    "hits = hits[0][0]\n",
    "max_pos = hits['corpus_id']\n",
    "max_score = hits['score']\n",
    "similar_answer = texts[max_pos]\n",
    "\n",
    "tokenized_query = query.split(\" \")\n",
    "answer_scores = bm25.get_scores(tokenized_query)\n",
    "max_score_bm25 = answer_scores.max()\n",
    "top_answer = bm25.get_top_n(tokenized_query, texts, n=1)[0]\n",
    "\n",
    "print(f\"Query: {query}\")\n",
    "print('------------- Similarity NN -------------')\n",
    "print(f\"Similarity score: {max_score*100:.2f}%\")\n",
    "print(f\"Context: {similar_answer}\")\n",
    "print('------------- Similarity BM25 -------------')\n",
    "print(f\"Similarity score: {max_score_bm25}\")\n",
    "print(f\"Top answer: {top_answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n",
      "Context: chemical this comprises forms of erosion suffered by the marble mainly as a result of acid rain. a serious problem connected with atmospheric pollution concerns the various accretions and deposits in the form of powder, carbon (black soot) and metal rust\n",
      "Similarity score: 74.22%\n",
      "\n",
      "649\n",
      "Context: “sugaring” or granural disaggregation of marble crystals appears on surface areas directly exposed to rain fall. the rainwater acidified by carbon dioxide, sulphur dioxide and nitrogen oxides from the air pollution slowly dissolves the marble\n",
      "Similarity score: 73.32%\n",
      "\n",
      "25\n",
      "Context: degradation by acids acids react with the calcium carbonate in marble, producing carbonic acid (which decomposes quickly to co2 and h2o) and other soluble salts : caco3(s) + 2h+(aq) → ca2+(aq) + co2(g) + h2o (l)outdoor marble statues, gravestones, or other marble structures are damaged by acid rain whether by carbonation, sulphation or the formation of \"blackcrust\" (accumulation of calcium sulphate, nitrates and carbon particles)\n",
      "Similarity score: 73.17%\n",
      "\n",
      "650\n",
      "Context: the rainwater acidified by carbon dioxide, sulphur dioxide and nitrogen oxides from the air pollution slowly dissolves the marble. this damage is evident in the effect on the marble crystals, leading to the loss of their cohesion and ultimately in their falling off\n",
      "Similarity score: 71.98%\n",
      "\n",
      "648\n",
      "Context: obviously, chemicals should not be used indiscriminately in towns as problems of elimination could arise with already polluted air .“sugaring” or granural disaggregation of marble crystals appears on surface areas directly exposed to rain fall\n",
      "Similarity score: 69.89%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for row in hits_np:\n",
    "    print(row['corpus_id'])\n",
    "    print(f\"Context: {texts[row['corpus_id']]}\")\n",
    "    print(f\"Similarity score: {row['score']*100:.2f}%\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Is kythnos a good island?\n",
      "Score: 22.48\n",
      "Context: one example is the oseberg viking ship, which was found embedded in waterlogged clay at a land site excavation in norway year 1902. another is the warship vasa, which was raised in 1961 after 333 years in the brackish and cold waters of the baltic sea \n",
      "\n",
      "We do not have such context in our knowledge base. Answering with AI without providing it with context, make sure to search the correct answer with critical thinking and research.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vmpletsos\\Anaconda3\\envs\\guide\\lib\\site-packages\\transformers\\pipelines\\base.py:1070: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The answer is: Yes, kythnos is a good island. In fact, kythnos is considered the most beautiful island in greece. It has a lot of beaches, amazing sceneries, and it is very popular for tourism.\n"
     ]
    }
   ],
   "source": [
    "query = \"Is kythnos a good island?\"\n",
    "tokenized_query = similarity_tokenizer(query, padding=True, truncation=True, return_tensors='pt')\n",
    "embedded_query = similarity_model(**tokenized_query)\n",
    "question_embeddings = mean_pooling(embedded_query, tokenized_query['attention_mask'])\n",
    "question_embeddings = question_embeddings.detach().numpy()\n",
    "scores = cosine_similarity([question_embeddings[0]], sentence_embeddings)[0]\n",
    "max_pos = np.argmax(scores[1:])\n",
    "max_score = scores[max_pos+1]\n",
    "context = texts[max_pos+1]\n",
    "\n",
    "print(f'Question: {query}')\n",
    "print(f'Score: {max_score*100:.2f}')\n",
    "print(f'Context: {context} \\n')\n",
    "query = 'Answer the following question only with the provided input. If no answer is found tell that you cannot answer based on this context.' + query;\n",
    "if max_score <= 0.4:\n",
    "    print(\"We do not have such context in our knowledge base. Answering with AI without providing it with context, make sure to search the correct answer with critical thinking and research.\")\n",
    "    print(llm_chain.predict(instruction=query).lstrip())\n",
    "elif max_score <= 0.65 and max_score > 4:\n",
    "    print(\"Sorry, i am not exactly sure based on my knowledge base, answering with very low confidence...\")\n",
    "    print(llm_context_chain.predict(instruction=query, context='context').lstrip())\n",
    "elif max_score <= 0.9 and max_score > 0.65:\n",
    "    print(\"Based on the knowledge from database, generating answer...\")\n",
    "    print(llm_context_chain.predict(instruction=query, context=context).lstrip())\n",
    "else:\n",
    "    print(\"Similar question was found with high confidence\")\n",
    "    print(f\"Answer: {context}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "guide",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
