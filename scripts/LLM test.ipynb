{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sys import getsizeof\n",
    "from transformers import AutoTokenizer, AutoModel, pipeline\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from rank_bm25 import BM25Okapi\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from llama_index import SimpleDirectoryReader\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSeq2SeqLM, pipeline\n",
    "from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer, DPRContextEncoder, DPRContextEncoderTokenizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load similarity model\n",
    "# similarity_tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "# similarity_model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "# print(\"Loaded similarity model!\")\n",
    "\n",
    "# # Mean Pooling - Take attention mask into account for correct averaging\n",
    "# def mean_pooling(model_output, attention_mask):\n",
    "#     # First element of model_output contains all token embeddings\n",
    "#     token_embeddings = model_output[0]\n",
    "#     input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "#     return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "# # Read the extracted questions csv\n",
    "# df = pd.read_csv('../actions/new_passages.csv')\n",
    "# passages = df.passages.to_list()\n",
    "# encoded_input = similarity_tokenizer(passages, padding=True, truncation=True, return_tensors='pt')\n",
    "# with torch.no_grad():\n",
    "#     model_output = similarity_model(**encoded_input)\n",
    "# # Perform pooling\n",
    "# sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "# sentence_embeddings = sentence_embeddings.detach().numpy()\n",
    "# print(\"Loaded knowledge base!\")\n",
    "\n",
    "\n",
    "# # Load similarity model\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n",
    "# print(\"Loaded tokenizer!\")\n",
    "# model = DPRContextEncoder.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n",
    "# # model.to(\"cuda\")\n",
    "# print(\"Loaded similarity model!\")\n",
    "\n",
    "# # Read the extracted questions csv\n",
    "# df = pd.read_csv('../actions/new_passages.csv')\n",
    "# passages = df.passages.to_list()\n",
    "# input_ids = tokenizer(passages, return_tensors=\"pt\")[\"input_ids\"]\n",
    "# input_ids = input_ids.to(\"cuda\")\n",
    "# sentence_embeddings = model(input_ids).pooler_output\n",
    "# print(\"Loaded knowledge base!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove with regex the brackets and its content from a phrase\n",
    "def remove_references(text):\n",
    "    # strip sentenece\n",
    "    text = text.lower().strip()\n",
    "    # remove strange characters from documents\n",
    "    text = text.replace('\\xad', '')\n",
    "    # remove dashes\n",
    "    text = text.replace('-', '')\n",
    "    # remove new lines\n",
    "    text = text.replace('\\n', '')\n",
    "    # remove strange combinations\n",
    "    text = text.replace('=-', '')\n",
    "    # remove brackets\n",
    "    text = re.sub(r'\\(\\d+\\)', '', text)\n",
    "    # remove figures\n",
    "    text = re.sub(r'\\(\\w+ \\d+\\)', '', text)\n",
    "    # remove references\n",
    "    return re.sub(r'\\[[\\d\\- ,]+\\]', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader('./Knowledge Base/').load_data();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [document.text for document in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(        \n",
    "    separator = \".\",\n",
    "    chunk_size = 300,\n",
    "    chunk_overlap  = 150,\n",
    "    length_function = len,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 312, which is longer than the specified 300\n",
      "Created a chunk of size 435, which is longer than the specified 300\n",
      "Created a chunk of size 353, which is longer than the specified 300\n",
      "Created a chunk of size 385, which is longer than the specified 300\n",
      "Created a chunk of size 316, which is longer than the specified 300\n",
      "Created a chunk of size 330, which is longer than the specified 300\n",
      "Created a chunk of size 314, which is longer than the specified 300\n",
      "Created a chunk of size 347, which is longer than the specified 300\n",
      "Created a chunk of size 355, which is longer than the specified 300\n",
      "Created a chunk of size 329, which is longer than the specified 300\n",
      "Created a chunk of size 320, which is longer than the specified 300\n",
      "Created a chunk of size 302, which is longer than the specified 300\n",
      "Created a chunk of size 378, which is longer than the specified 300\n",
      "Created a chunk of size 340, which is longer than the specified 300\n",
      "Created a chunk of size 301, which is longer than the specified 300\n",
      "Created a chunk of size 309, which is longer than the specified 300\n",
      "Created a chunk of size 366, which is longer than the specified 300\n",
      "Created a chunk of size 320, which is longer than the specified 300\n",
      "Created a chunk of size 454, which is longer than the specified 300\n",
      "Created a chunk of size 384, which is longer than the specified 300\n",
      "Created a chunk of size 304, which is longer than the specified 300\n",
      "Created a chunk of size 324, which is longer than the specified 300\n",
      "Created a chunk of size 320, which is longer than the specified 300\n"
     ]
    }
   ],
   "source": [
    "texts = text_splitter.create_documents(corpus);\n",
    "texts = [remove_references(sentence.page_content) for sentence in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'passages': texts})\n",
    "df.to_csv('new_passages.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('new_passages.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model from HuggingFace Hub\n",
    "similarity_tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "similarity_model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# questions = df.question.to_list()\n",
    "# Tokenize sentences\n",
    "encoded_input = similarity_tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n",
    "with torch.no_grad():\n",
    "    model_output = similarity_model(**encoded_input)\n",
    "\n",
    "# Perform pooling\n",
    "sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "sentence_embeddings = sentence_embeddings.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentences = [text.split(\" \") for text in texts]\n",
    "bm25 = BM25Okapi(tokenized_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM = pipeline(\n",
    "    model=\"databricks/dolly-v2-3b\", \n",
    "    torch_dtype=torch.bfloat16, \n",
    "    trust_remote_code=True,\n",
    "    return_full_text=True,\n",
    "    device_map=\"auto\",\n",
    "    task=\"text-generation\",\n",
    "    temperature=0.9,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# template for an instrution with no input\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"instruction\"],\n",
    "    template=\"{instruction}\")\n",
    "\n",
    "# template for an instruction with input\n",
    "prompt_with_context = PromptTemplate(\n",
    "    input_variables=[\"instruction\", \"context\"],\n",
    "    template=\"{instruction}\\n\\nInput:\\n{context}\")\n",
    "\n",
    "hf_pipeline = HuggingFacePipeline(pipeline=LLM)\n",
    "\n",
    "llm_chain = LLMChain(llm=hf_pipeline, prompt=prompt)\n",
    "llm_context_chain = LLMChain(llm=hf_pipeline, prompt=prompt_with_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Does rain affect the integrity of stone?\n",
      "------------- Similarity NN -------------\n",
      "Similarity score: 71.79%\n",
      "Context: in areas where the rainwater is relatively free from pollutants, the dissolution of most common building stones is usually not a serious problem .frost damage certain stones which are exposed to freezing temperatures and wet conditions may undergo frost damage\n",
      "------------- Similarity BM25 -------------\n",
      "Similarity score: 10.866191278608332\n",
      "Top answer: the processes involved in glass disease can reduce the transparency of the glass or even threaten the integrity of the structure\n"
     ]
    }
   ],
   "source": [
    "query = \"Does rain affect the integrity of stone?\"\n",
    "tokenized_query = similarity_tokenizer(query, padding=True, truncation=True, return_tensors='pt')\n",
    "embedded_query = similarity_model(**tokenized_query)\n",
    "question_embeddings = mean_pooling(embedded_query, tokenized_query['attention_mask'])\n",
    "question_embeddings = question_embeddings.detach().numpy()\n",
    "scores = cosine_similarity([question_embeddings[0]], sentence_embeddings)[0]\n",
    "max_pos = np.argmax(scores[1:])\n",
    "max_score = scores[max_pos+1]\n",
    "similar_answer = texts[max_pos+1]\n",
    "\n",
    "tokenized_query = query.split(\" \")\n",
    "answer_scores = bm25.get_scores(tokenized_query)\n",
    "max_score_bm25 = answer_scores.max()\n",
    "top_answer = bm25.get_top_n(tokenized_query, texts, n=1)[0]\n",
    "\n",
    "print(f\"Query: {query}\")\n",
    "print('------------- Similarity NN -------------')\n",
    "print(f\"Similarity score: {max_score*100:.2f}%\")\n",
    "print(f\"Context: {similar_answer}\")\n",
    "print('------------- Similarity BM25 -------------')\n",
    "print(f\"Similarity score: {max_score_bm25}\")\n",
    "print(f\"Top answer: {top_answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Is kythnos a good island?\n",
      "Score: 22.48\n",
      "Context: one example is the oseberg viking ship, which was found embedded in waterlogged clay at a land site excavation in norway year 1902. another is the warship vasa, which was raised in 1961 after 333 years in the brackish and cold waters of the baltic sea \n",
      "\n",
      "We do not have such context in our knowledge base. Answering with AI without providing it with context, make sure to search the correct answer with critical thinking and research.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vmpletsos\\Anaconda3\\envs\\guide\\lib\\site-packages\\transformers\\pipelines\\base.py:1070: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The answer is: Yes, kythnos is a good island. In fact, kythnos is considered the most beautiful island in greece. It has a lot of beaches, amazing sceneries, and it is very popular for tourism.\n"
     ]
    }
   ],
   "source": [
    "query = \"Is kythnos a good island?\"\n",
    "tokenized_query = similarity_tokenizer(query, padding=True, truncation=True, return_tensors='pt')\n",
    "embedded_query = similarity_model(**tokenized_query)\n",
    "question_embeddings = mean_pooling(embedded_query, tokenized_query['attention_mask'])\n",
    "question_embeddings = question_embeddings.detach().numpy()\n",
    "scores = cosine_similarity([question_embeddings[0]], sentence_embeddings)[0]\n",
    "max_pos = np.argmax(scores[1:])\n",
    "max_score = scores[max_pos+1]\n",
    "context = texts[max_pos+1]\n",
    "\n",
    "print(f'Question: {query}')\n",
    "print(f'Score: {max_score*100:.2f}')\n",
    "print(f'Context: {context} \\n')\n",
    "query = 'Answer the following question only with the provided input. If no answer is found tell that you cannot answer based on this context.' + query;\n",
    "if max_score <= 0.4:\n",
    "    print(\"We do not have such context in our knowledge base. Answering with AI without providing it with context, make sure to search the correct answer with critical thinking and research.\")\n",
    "    print(llm_chain.predict(instruction=query).lstrip())\n",
    "elif max_score <= 0.65 and max_score > 4:\n",
    "    print(\"Sorry, i am not exactly sure based on my knowledge base, answering with very low confidence...\")\n",
    "    print(llm_context_chain.predict(instruction=query, context='context').lstrip())\n",
    "elif max_score <= 0.9 and max_score > 0.65:\n",
    "    print(\"Based on the knowledge from database, generating answer...\")\n",
    "    print(llm_context_chain.predict(instruction=query, context=context).lstrip())\n",
    "else:\n",
    "    print(\"Similar question was found with high confidence\")\n",
    "    print(f\"Answer: {context}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "guide",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
