{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vmpletsos\\Anaconda3\\envs\\guide\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, pipeline\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from rank_bm25 import BM25Okapi\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from llama_index import SimpleDirectoryReader\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSeq2SeqLM, pipeline\n",
    "from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove with regex the brackets and its content from a phrase\n",
    "def remove_references(text):\n",
    "    # strip sentenece\n",
    "    text = text.lower().strip()\n",
    "    # remove strange characters from documents\n",
    "    text = text.replace('\\xad', '')\n",
    "    # remove dashes\n",
    "    text = text.replace('-', '')\n",
    "    # remove new lines\n",
    "    text = text.replace('\\n', '')\n",
    "    # remove strange combinations\n",
    "    text = text.replace('=-', '')\n",
    "    # remove brackets\n",
    "    text = re.sub(r'\\(\\d+\\)', '', text)\n",
    "    # remove figures\n",
    "    text = re.sub(r'\\(\\w+ \\d+\\)', '', text)\n",
    "    # remove references\n",
    "    return re.sub(r'\\[[\\d\\- ,]+\\]', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader('./Knowledge Base/').load_data();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [document.text for document in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(        \n",
    "    separator = \".\",\n",
    "    chunk_size = 300,\n",
    "    chunk_overlap  = 150,\n",
    "    length_function = len,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 312, which is longer than the specified 300\n",
      "Created a chunk of size 435, which is longer than the specified 300\n",
      "Created a chunk of size 353, which is longer than the specified 300\n",
      "Created a chunk of size 385, which is longer than the specified 300\n",
      "Created a chunk of size 316, which is longer than the specified 300\n",
      "Created a chunk of size 330, which is longer than the specified 300\n",
      "Created a chunk of size 314, which is longer than the specified 300\n",
      "Created a chunk of size 347, which is longer than the specified 300\n",
      "Created a chunk of size 355, which is longer than the specified 300\n",
      "Created a chunk of size 329, which is longer than the specified 300\n",
      "Created a chunk of size 320, which is longer than the specified 300\n",
      "Created a chunk of size 302, which is longer than the specified 300\n",
      "Created a chunk of size 378, which is longer than the specified 300\n",
      "Created a chunk of size 340, which is longer than the specified 300\n",
      "Created a chunk of size 301, which is longer than the specified 300\n",
      "Created a chunk of size 309, which is longer than the specified 300\n",
      "Created a chunk of size 366, which is longer than the specified 300\n",
      "Created a chunk of size 320, which is longer than the specified 300\n",
      "Created a chunk of size 454, which is longer than the specified 300\n",
      "Created a chunk of size 384, which is longer than the specified 300\n",
      "Created a chunk of size 304, which is longer than the specified 300\n",
      "Created a chunk of size 324, which is longer than the specified 300\n",
      "Created a chunk of size 320, which is longer than the specified 300\n"
     ]
    }
   ],
   "source": [
    "texts = text_splitter.create_documents(corpus);\n",
    "texts = [remove_references(sentence.page_content) for sentence in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'passages': texts})\n",
    "df.to_csv('new_passages.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('new_passages.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model from HuggingFace Hub\n",
    "similarity_tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "similarity_model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# questions = df.question.to_list()\n",
    "# Tokenize sentences\n",
    "encoded_input = similarity_tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n",
    "with torch.no_grad():\n",
    "    model_output = similarity_model(**encoded_input)\n",
    "\n",
    "# Perform pooling\n",
    "sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "sentence_embeddings = sentence_embeddings.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentences = [text.split(\" \") for text in texts]\n",
    "bm25 = BM25Okapi(tokenized_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM = pipeline(\n",
    "    model=\"databricks/dolly-v2-3b\", \n",
    "    torch_dtype=torch.bfloat16, \n",
    "    trust_remote_code=True,\n",
    "    return_full_text=True,\n",
    "    device_map=\"auto\",\n",
    "    task=\"text-generation\",\n",
    "    temperature=0.9,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# template for an instrution with no input\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"instruction\"],\n",
    "    template=\"{instruction}\")\n",
    "\n",
    "# template for an instruction with input\n",
    "prompt_with_context = PromptTemplate(\n",
    "    input_variables=[\"instruction\", \"context\"],\n",
    "    template=\"{instruction}\\n\\nInput:\\n{context}\")\n",
    "\n",
    "hf_pipeline = HuggingFacePipeline(pipeline=LLM)\n",
    "\n",
    "llm_chain = LLMChain(llm=hf_pipeline, prompt=prompt)\n",
    "llm_context_chain = LLMChain(llm=hf_pipeline, prompt=prompt_with_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Is marble a stron material\n",
      "------------- Similarity NN -------------\n",
      "Similarity score: 62.08%\n",
      "Context: the hardness of marble is very high, because the internal structure of the rock is very uniform after longterm natural aging, and the internal stress disappears, so the marble will not be deformed due to temperature, and has strong wear resistance. it is a very popular building material\n",
      "------------- Similarity BM25 -------------\n",
      "Similarity score: 8.048500491959693\n",
      "Top answer: marble is typically not foliated (layered), although there are exceptions.in geology, the term marble refers to metamorphosed limestone, but its use in stonemasonry more broadly encompasses unmetamorphosed limestone. marble is commonly used for sculpture and as a building material\n"
     ]
    }
   ],
   "source": [
    "query = \"Is marble a stron material\"\n",
    "tokenized_query = similarity_tokenizer(query, padding=True, truncation=True, return_tensors='pt')\n",
    "embedded_query = similarity_model(**tokenized_query)\n",
    "question_embeddings = mean_pooling(embedded_query, tokenized_query['attention_mask'])\n",
    "question_embeddings = question_embeddings.detach().numpy()\n",
    "scores = cosine_similarity([question_embeddings[0]], sentence_embeddings)[0]\n",
    "max_pos = np.argmax(scores[1:])\n",
    "max_score = scores[max_pos+1]\n",
    "similar_answer = texts[max_pos+1]\n",
    "\n",
    "tokenized_query = query.split(\" \")\n",
    "answer_scores = bm25.get_scores(tokenized_query)\n",
    "max_score_bm25 = answer_scores.max()\n",
    "top_answer = bm25.get_top_n(tokenized_query, texts, n=1)[0]\n",
    "\n",
    "print(f\"Query: {query}\")\n",
    "print('------------- Similarity NN -------------')\n",
    "print(f\"Similarity score: {max_score*100:.2f}%\")\n",
    "print(f\"Context: {similar_answer}\")\n",
    "print('------------- Similarity BM25 -------------')\n",
    "print(f\"Similarity score: {max_score_bm25}\")\n",
    "print(f\"Top answer: {top_answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Where there are cracks there is danger?\n",
      "Score: 60.29\n",
      "Context: most of the cracks have been caused by the rusting and swelling of the iron reinforcements, mainly from the balanos and orlandos restoration . there are also fractures that are the result of mechanical forces or heat (such as earthquakes, the fire in antiquity, and the explosion of 1687) \n",
      "\n",
      "Similar question was found with high confidence\n",
      "Answer: most of the cracks have been caused by the rusting and swelling of the iron reinforcements, mainly from the balanos and orlandos restoration . there are also fractures that are the result of mechanical forces or heat (such as earthquakes, the fire in antiquity, and the explosion of 1687)\n"
     ]
    }
   ],
   "source": [
    "query = \"Where there are cracks there is danger?\"\n",
    "tokenized_query = similarity_tokenizer(query, padding=True, truncation=True, return_tensors='pt')\n",
    "embedded_query = similarity_model(**tokenized_query)\n",
    "question_embeddings = mean_pooling(embedded_query, tokenized_query['attention_mask'])\n",
    "question_embeddings = question_embeddings.detach().numpy()\n",
    "scores = cosine_similarity([question_embeddings[0]], sentence_embeddings)[0]\n",
    "max_pos = np.argmax(scores[1:])\n",
    "max_score = scores[max_pos+1]\n",
    "context = texts[max_pos+1]\n",
    "\n",
    "print(f'Question: {query}')\n",
    "print(f'Score: {max_score*100:.2f}')\n",
    "print(f'Context: {context} \\n')\n",
    "if max_score <= 0.4:\n",
    "    print(\"We do not have such context in our knowledge base. Answering with AI without providing it with context, make sure to search the correct answer with critical thinking and research.\")\n",
    "    print(llm_chain.predict(instruction=query).lstrip())\n",
    "elif max_score <= 0.65 and max_score > 4:\n",
    "    print(\"Sorry, i am not exactly sure based on my knowledge base, answering with very low confidence...\")\n",
    "    print(llm_context_chain.predict(instruction=query, context='context').lstrip())\n",
    "elif max_score <= 0.9 and max_score > 0.65:\n",
    "    print(\"Based on the knowledge from database, generating answer...\")\n",
    "    print(llm_context_chain.predict(instruction=query, context=context).lstrip())\n",
    "else:\n",
    "    print(\"Similar question was found with high confidence\")\n",
    "    print(f\"Answer: {context}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yes, kythnos is a beautiful island. The island has many striking features such as black volcanic soil that gives the island a unique look, whitewashed houses, azure seas, lush green vegetation and, of course, beautiful whitewashed houses.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = 'Is kythnos a beautiful island?'\n",
    "llm_chain.predict(instruction=query).lstrip()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "guide",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
