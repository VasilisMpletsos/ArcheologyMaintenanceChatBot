{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vmpletsos\\Anaconda3\\envs\\guide\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, pipeline\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from rank_bm25 import BM25Okapi\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from llama_index import SimpleDirectoryReader\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSeq2SeqLM, pipeline\n",
    "from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove with regex the brackets and its content from a phrase\n",
    "def remove_references(text):\n",
    "    # strip sentenece\n",
    "    text = text.lower().strip()\n",
    "    # remove strange characters from documents\n",
    "    text = text.replace('\\xad', '')\n",
    "    # remove dashes\n",
    "    text = text.replace('-', '')\n",
    "    # remove new lines\n",
    "    text = text.replace('\\n', '')\n",
    "    # remove strange combinations\n",
    "    text = text.replace('=-', '')\n",
    "    # remove brackets\n",
    "    text = re.sub(r'\\(\\d+\\)', '', text)\n",
    "    # remove figures\n",
    "    text = re.sub(r'\\(\\w+ \\d+\\)', '', text)\n",
    "    # remove references\n",
    "    return re.sub(r'\\[[\\d\\- ,]+\\]', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader('./Knowledge Base/').load_data();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [document.text for document in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(        \n",
    "    separator = \".\",\n",
    "    chunk_size = 300,\n",
    "    chunk_overlap  = 150,\n",
    "    length_function = len,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 312, which is longer than the specified 300\n",
      "Created a chunk of size 435, which is longer than the specified 300\n",
      "Created a chunk of size 353, which is longer than the specified 300\n",
      "Created a chunk of size 385, which is longer than the specified 300\n",
      "Created a chunk of size 316, which is longer than the specified 300\n",
      "Created a chunk of size 330, which is longer than the specified 300\n",
      "Created a chunk of size 314, which is longer than the specified 300\n",
      "Created a chunk of size 347, which is longer than the specified 300\n",
      "Created a chunk of size 355, which is longer than the specified 300\n",
      "Created a chunk of size 329, which is longer than the specified 300\n",
      "Created a chunk of size 320, which is longer than the specified 300\n",
      "Created a chunk of size 302, which is longer than the specified 300\n",
      "Created a chunk of size 378, which is longer than the specified 300\n",
      "Created a chunk of size 340, which is longer than the specified 300\n",
      "Created a chunk of size 301, which is longer than the specified 300\n",
      "Created a chunk of size 309, which is longer than the specified 300\n",
      "Created a chunk of size 366, which is longer than the specified 300\n",
      "Created a chunk of size 320, which is longer than the specified 300\n",
      "Created a chunk of size 454, which is longer than the specified 300\n",
      "Created a chunk of size 384, which is longer than the specified 300\n",
      "Created a chunk of size 304, which is longer than the specified 300\n",
      "Created a chunk of size 324, which is longer than the specified 300\n",
      "Created a chunk of size 320, which is longer than the specified 300\n"
     ]
    }
   ],
   "source": [
    "texts = text_splitter.create_documents(corpus);\n",
    "texts = [remove_references(sentence.page_content) for sentence in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'passages': texts})\n",
    "df.to_csv('new_passages.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('new_passages.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model from HuggingFace Hub\n",
    "similarity_tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "similarity_model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# questions = df.question.to_list()\n",
    "# Tokenize sentences\n",
    "encoded_input = similarity_tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n",
    "with torch.no_grad():\n",
    "    model_output = similarity_model(**encoded_input)\n",
    "\n",
    "# Perform pooling\n",
    "sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "sentence_embeddings = sentence_embeddings.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentences = [text.split(\" \") for text in texts]\n",
    "bm25 = BM25Okapi(tokenized_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM = pipeline(\n",
    "    model=\"databricks/dolly-v2-3b\", \n",
    "    torch_dtype=torch.bfloat16, \n",
    "    trust_remote_code=True,\n",
    "    return_full_text=True,\n",
    "    device_map=\"auto\",\n",
    "    task=\"text-generation\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# template for an instrution with no input\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"instruction\"],\n",
    "    template=\"{instruction}\")\n",
    "\n",
    "# template for an instruction with input\n",
    "prompt_with_context = PromptTemplate(\n",
    "    input_variables=[\"instruction\", \"context\"],\n",
    "    template=\"{instruction}\\n\\nInput:\\n{context}\")\n",
    "\n",
    "hf_pipeline = HuggingFacePipeline(pipeline=LLM)\n",
    "\n",
    "llm_chain = LLMChain(llm=hf_pipeline, prompt=prompt)\n",
    "llm_context_chain = LLMChain(llm=hf_pipeline, prompt=prompt_with_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: How to maintain marble with cracks\n",
      "------------- Similarity NN -------------\n",
      "Similarity score: 69.20%\n",
      "Similar question: what is used to clean marble surfaces?\n",
      "Context: a mild, phneutral, nonabrasive soap should be used for cleaning marble surfaces. wipe with a soft foam cotton or rag.salt crystallization crystallization of salts within the pores of stones can generate sufficient stresses to cause the cracking of stone, often into powder fragments\n",
      "------------- Similarity BM25 -------------\n",
      "Similarity score: 7.432180800723318\n",
      "Top answer: old plaster removal after consolidation, older mortars that have failed or have caused cracks and other sideeffects are removed by mechanical means. the plaster is not removed in parts of the surface, where they can cause further damage to marble\n"
     ]
    }
   ],
   "source": [
    "query = \"How to maintain marble with cracks\"\n",
    "tokenized_query = similarity_tokenizer(query, padding=True, truncation=True, return_tensors='pt')\n",
    "embedded_query = similarity_model(**tokenized_query)\n",
    "question_embeddings = mean_pooling(embedded_query, tokenized_query['attention_mask'])\n",
    "question_embeddings = question_embeddings.detach().numpy()\n",
    "scores = cosine_similarity([question_embeddings[0]], sentence_embeddings)[0]\n",
    "max_pos = np.argmax(scores[1:])\n",
    "max_score = scores[max_pos+1]\n",
    "similar_question = texts[max_pos+1]\n",
    "\n",
    "tokenized_query = query.split(\" \")\n",
    "answer_scores = bm25.get_scores(tokenized_query)\n",
    "max_score_bm25 = answer_scores.max()\n",
    "top_answer = bm25.get_top_n(tokenized_query, texts, n=1)[0]\n",
    "\n",
    "print(f\"Query: {query}\")\n",
    "print('------------- Similarity NN -------------')\n",
    "print(f\"Similarity score: {max_score*100:.2f}%\")\n",
    "print(f\"Similar question: {similar_question}\")\n",
    "print(f\"Context: {texts[max_pos+1]}\")\n",
    "print('------------- Similarity BM25 -------------')\n",
    "print(f\"Similarity score: {max_score_bm25}\")\n",
    "print(f\"Top answer: {top_answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What creates sulfation?\n",
      "Score: 50.42\n",
      "Context: for example, limestone can react with sulfur dioxide to ultimately produce calcium sulfate. other sources of salts include ground water , airborne salts , sea spray , chemical cleaners , and deicing salts .aqueous dissolution carbonate sedimentary stones e.g \n",
      "\n",
      "Similar question was found with high confidence\n",
      "Answer: for example, limestone can react with sulfur dioxide to ultimately produce calcium sulfate. other sources of salts include ground water , airborne salts , sea spray , chemical cleaners , and deicing salts .aqueous dissolution carbonate sedimentary stones e.g\n"
     ]
    }
   ],
   "source": [
    "query = \"What creates sulfation?\"\n",
    "tokenized_query = similarity_tokenizer(query, padding=True, truncation=True, return_tensors='pt')\n",
    "embedded_query = similarity_model(**tokenized_query)\n",
    "question_embeddings = mean_pooling(embedded_query, tokenized_query['attention_mask'])\n",
    "question_embeddings = question_embeddings.detach().numpy()\n",
    "scores = cosine_similarity([question_embeddings[0]], sentence_embeddings)[0]\n",
    "max_pos = np.argmax(scores[1:])\n",
    "max_score = scores[max_pos+1]\n",
    "context = texts[max_pos+1]\n",
    "\n",
    "print(f'Question: {query}')\n",
    "print(f'Score: {max_score*100:.2f}')\n",
    "print(f'Context: {context} \\n')\n",
    "if max_score <= 0.4:\n",
    "    print(\"We do not have such context in our knowledge base. Answering with AI without providing it with context, make sure to search the correct answer with critical thinking and research.\")\n",
    "    print(llm_chain.predict(instruction=query).lstrip())\n",
    "elif max_score <= 0.65 and max_score > 4:\n",
    "    print(\"Sorry, i am not exactly sure based on my knowledge base, answering with very low confidence...\")\n",
    "    print(llm_context_chain.predict(instruction=query, context='context').lstrip())\n",
    "elif max_score <= 0.9 and max_score > 0.65:\n",
    "    print(\"Based on the knowledge from database, generating answer...\")\n",
    "    print(llm_context_chain.predict(instruction=query, context=context).lstrip())\n",
    "else:\n",
    "    print(\"Similar question was found with high confidence\")\n",
    "    print(f\"Answer: {context}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yes, Kythnos is an extremely beautiful island. People travel from all over the world to visit Kythnos and the beautiful island weather, scenic views, delicious food, and friendly people of the locals makes it a must visit island.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = 'Is kythnos a beautiful island?'\n",
    "llm_chain.predict(instruction=query).lstrip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vmpletsos\\Anaconda3\\envs\\guide\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading: 100%|██████████| 2.54k/2.54k [00:00<00:00, 1.27MB/s]\n",
      "Downloading: 100%|██████████| 792k/792k [00:00<00:00, 2.33MB/s]\n",
      "Downloading: 100%|██████████| 2.42M/2.42M [00:00<00:00, 2.93MB/s]\n",
      "Downloading: 100%|██████████| 2.20k/2.20k [00:00<00:00, 2.20MB/s]\n",
      "Downloading: 100%|██████████| 662/662 [00:00<00:00, 662kB/s]\n",
      "Downloading: 100%|██████████| 3.13G/3.13G [01:12<00:00, 43.0MB/s] \n",
      "Downloading: 100%|██████████| 147/147 [00:00<00:00, 147kB/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-large\")\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "guide",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
